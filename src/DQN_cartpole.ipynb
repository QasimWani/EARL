{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model-Aware RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Box2D\n",
    "# !pip install 'gym[all]'\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import DQNetwork\n",
    "from agent import Agent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space #continuous with 4 observations for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space #discrete with 2 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an agent using random policy Ï€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 18.0\n",
      "Iteration #: 17\n",
      "Ending state:\n",
      "[-0.08654556 -1.14907721  0.22016025  2.08093367]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    #env.render() #sudo-human\n",
    "    if(done):\n",
    "        break\n",
    "print(\"Reward: {}\\nIteration #: {}\\nEnding state:\\n{}\".format(total_reward, i, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "#To load an agent\n",
    "# actor.load_model(\"../model/policy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []  # List with all scores per episode\n",
    "score_d = deque(maxlen=100) #Last 100 episodes\n",
    "NUM_EPISODES = 5_000\n",
    "ENV_SOLVED = 175 #How many mean iterations to stay 'alive' in order to succeed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_i(num_episode, epsilon_min = 0.01):\n",
    "    \"\"\"Simple Epsilon Decay over total number of episodes. Stochastic in nature when summed over\"\"\"\n",
    "    epsilon = 1.0/num_episode\n",
    "    return max(epsilon, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800: Score: 200.0; Last 100 mean: 173.13; Epsilon: 0.01\n",
      "\n",
      "Solved at episode 1802 with score: 200.0 and mean: 176.82\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPISODES+1):\n",
    "    env_info = env.reset()   # reset the environment\n",
    "    state = env.reset()      # get the initial state\n",
    "    score = 0                # initialize the score\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        epsilon = get_epsilon_i(epoch)\n",
    "        action = actor.get_action(state, epsilon)              # select an action\n",
    "        next_state, reward, done, _ = env.step(action)         # step into next state\n",
    "        transition = (state, action, reward, next_state, done) # set transition\n",
    "        actor.step(transition)                                 # Train the model\n",
    "        score += reward                                        # update the score\n",
    "        state = next_state                                     # roll over the state to next time step\n",
    "        if done:                                               # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "    score_d.append(score)\n",
    "    \n",
    "    if(epoch%50 == 0):#Print stats every 50 episodes\n",
    "        print(f\"\\r{epoch}: Score: {score}; Last 100 mean: {np.mean(score_d)}; Epsilon: {epsilon}\", end=\"\")\n",
    "    if(np.mean(score_d) >= ENV_SOLVED):\n",
    "        print(f\"\\n\\nSolved at episode {epoch} with score: {score} and mean: {np.mean(score_d)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from agent import WEIGHT_STATE\n",
    "print(WEIGHT_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = 1833, 176.68\n",
    "# 0.1 = 1918, 175.47\n",
    "# 0.4 = 2434, 176.26\n",
    "# 0.8 = 2876, 175.26\n",
    "# 0.95 = 2627, 175.78\n",
    "# 1 = not converged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
